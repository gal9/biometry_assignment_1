{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from src.data import read_grayscale_image\n",
    "from src.features import flatten_image\n",
    "from src.recognition import euclidian_distance_recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading image data/awe\\100\\10.png\r"
     ]
    }
   ],
   "source": [
    "images_grayscale = []\n",
    "images_flattened = []\n",
    "\n",
    "data_location = \"data/awe\"\n",
    "\n",
    "# Loop through data directory\n",
    "for filename in os.listdir(data_location):\n",
    "    f = os.path.join(data_location, filename)\n",
    "\n",
    "    if os.path.isdir(f):\n",
    "        grayscalles_tmp = []\n",
    "        flattened_tmp = []\n",
    "\n",
    "        for image_file in os.listdir(f):\n",
    "            image_location = os.path.join(f, image_file)\n",
    "\n",
    "            if(image_file.endswith(\".png\")):\n",
    "                print(f\"Reading image {image_location}\", end=\"\\r\")\n",
    "\n",
    "                # Reading grayscale image and transforming it to 1D vector\n",
    "                image_grayscale = read_grayscale_image(image_location, 128, 128)\n",
    "                image_flattened = flatten_image(image_grayscale)\n",
    "\n",
    "                grayscalles_tmp.append(image_grayscale)\n",
    "                flattened_tmp.append(image_flattened)\n",
    "        \n",
    "        images_grayscale.append(grayscalles_tmp)\n",
    "        images_flattened.append(flattened_tmp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55rsone 6/100\r"
     ]
    }
   ],
   "source": [
    "TP = 0\n",
    "c = 0\n",
    "number_of_people = len(images_flattened)\n",
    "number_of_samples = len(images_flattened)*len(images_flattened[0])\n",
    "\n",
    "for persone_index, samples in enumerate(images_flattened):\n",
    "    #print(f\"Persone {persone_index+1}/{number_of_people}\", end=\"\\r\")\n",
    "    for sample_i, sample in enumerate(samples):\n",
    "        c += 1\n",
    "        print(f\"Sample {c}/{number_of_samples}\", end=\"\\r\")\n",
    "        persone_recognised_i, image_recognised_i = euclidian_distance_recognition(sample, images_flattened, persone_index, sample_i)\n",
    "\n",
    "        # Test if image was correctly classfied\n",
    "        if(persone_index==persone_recognised_i):\n",
    "            # print(persone_recognised_i)\n",
    "            TP += 1\n",
    "\n",
    "print()\n",
    "\n",
    "print(TP/number_of_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5c13dbc957bb8477a7893c79680d2a6cafd4dd4044001ad54528252da2651a6e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
